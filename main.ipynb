{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing initial packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Setting the device to GPU for parallelization\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Cleaning, and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my project folder I added a data folder with both files (found in repo README) but did not push (gitignore) due to size\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "\n",
    "ratings = pd.read_csv('data/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"The shape of movies is: {movies.shape}\n",
    "The shape of rating is: {ratings.shape}\"\"\")\n",
    "\n",
    "print(f\"\"\"The columns of movies is: {movies.columns.to_list()}\n",
    "The columns of rating is: {ratings.columns.to_list()}\n",
    "\n",
    "* Notice that both df's have a movieId column, which will be useful for merging\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is an already cleaned dataset, we can skip over to data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we will first need to create a user-movie interaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume NAs are not watched by user, so rating of 0\n",
    "user_rating_matrix = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_rating_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this from a pandas df to numpy array\n",
    "user_rating_matrix_np = user_rating_matrix.values\n",
    "print(f\"Shape of the numpy matrix: {user_rating_matrix_np.shape}; represting {user_rating_matrix_np.shape[0]} users with {user_rating_matrix_np.shape[1]} movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test split\n",
    "train_data, test_data = train_test_split(user_rating_matrix_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "test_data = torch.FloatTensor(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Recommender model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, n_embd = 50):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, n_embd) # Embedding the users\n",
    "        self.item_embedding = nn.Embedding(num_items, n_embd) # Embedding items\n",
    "        self.fc1 = nn.Linear(n_embd * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embed = self.user_embedding(user)\n",
    "        item_embed = self.item_embedding(item)\n",
    "        x = torch.cat([user_embed, item_embed], dim=-1) # Concatenating the users and items \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "num_users, num_items = user_rating_matrix_np.shape\n",
    "model = Recommender(num_users, num_items).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer\n",
    "Purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.nonzero(as_tuple=True) # Get the indices of non-zero elements\n",
    "        self.ratings = data[self.data] # Using non-zero indiced to extract those ratings\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        user = self.data[0][idx]\n",
    "        item = self.data[1][idx]\n",
    "        rating = self.ratings[idx]\n",
    "        \n",
    "        return user, item, rating\n",
    "    \n",
    "train_dataset = RatingsDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "test_dataset = RatingsDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of samples in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test_dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our model, and prepared the data for training, we can move onto model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for user, item, rating in train_loader:\n",
    "        user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user, item).squeeze()\n",
    "        loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {(total_loss/len(train_loader))}\")  # Print the average loss for this epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in data_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            output = model(user, item).squeeze()\n",
    "            loss = criterion(output, rating)\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)   \n",
    "\n",
    "test_loss = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
