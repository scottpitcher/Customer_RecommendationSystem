{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing initial packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting the device to GPU for parallelization\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Cleaning, and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my project folder I added a data folder with both files (found in repo README) but did not push (gitignore) due to size\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "\n",
    "ratings = pd.read_csv('data/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"The shape of movies is: {movies.shape}\n",
    "The shape of rating is: {ratings.shape}\"\"\")\n",
    "\n",
    "print(f\"\"\"The columns of movies is: {movies.columns.to_list()}\n",
    "The columns of rating is: {ratings.columns.to_list()}\n",
    "\n",
    "* Notice that both df's have a movieId column, which will be useful for merging\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating encoders and decoders for our movie ids\n",
    "movie_ids = list(movies.movieId)\n",
    "moviesid_to_title = dict(zip(movies.movieId,movies.title))    # Decoder\n",
    "movietitle_to_id = {j:i for i,j in moviesid_to_title.items()} # Encoder\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ratings.rating, bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is an already cleaned dataset, we can skip over to data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we will first need to create a user-movie interaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume NAs are not watched by user, so rating of 0\n",
    "user_rating_matrix = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_rating_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this from a pandas df to numpy array\n",
    "user_rating_matrix_np = user_rating_matrix.values\n",
    "print(f\"Shape of the numpy matrix: {user_rating_matrix_np.shape}; represting {user_rating_matrix_np.shape[0]} users with {user_rating_matrix_np.shape[1]} movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test split\n",
    "train_data, test_data = train_test_split(user_rating_matrix_np, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "test_data = torch.FloatTensor(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.nonzero(as_tuple=True) # Get the indices of non-zero elements\n",
    "        self.ratings = data[self.data]          # Using non-zero indiced to extract those ratings\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        user = self.data[0][idx]\n",
    "        item = self.data[1][idx]\n",
    "        rating = self.ratings[idx]\n",
    "        \n",
    "        return user, item, rating\n",
    "    \n",
    "train_dataset = RatingsDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "test_dataset = RatingsDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of samples in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test_dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Recommender model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, n_embd = 64):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, n_embd) # Creating an embedding for our users with the correct dimensions\n",
    "        self.item_embedding = nn.Embedding(num_items, n_embd) # Embedding items\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_embd * 2, 128)                 # First fully connected (fc) layer\n",
    "        self.fc2 = nn.Linear(128, 64)                         # Second fc layer\n",
    "        self.fc3 = nn.Linear(64, 16)                          # Finaly fc layer, with an output of one value\n",
    "        self.fc4 = nn.Linear(64, 1)                           # Finaly fc layer, with an output of one value\n",
    "       \n",
    "        self.sigmoid = nn.Sigmoid()                           # Sigmoid activation function to compress the output to a value between (0,1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)                       # Dropout layer to prevent overfitting\n",
    "        self.dropout2 = nn.Dropout(0.4)                       # Dropout layer to prevent overfitting\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embed = self.user_embedding(user)          # Embedding our users\n",
    "        item_embed = self.item_embedding(item)          # Embedding the movies\n",
    "        \n",
    "        x = torch.cat([user_embed, item_embed], dim=-1) # Concatenating the users and items \n",
    "        x = torch.relu(self.fc1(x))                     # Applying first fc layer, with a ReLU activation function\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        # x = torch.relu(self.fc3(x))\n",
    "        # x = self.bn3(x)\n",
    "        x = self.fc4(x)                                 # Final layer to get predicted rating\n",
    "        x = self.sigmoid(x)                             # Applying the sigmoid function\n",
    "        x = x*5                                         # Scale the output from [0,1] to [0,5]   \n",
    "        return x\n",
    "\n",
    "\n",
    "# Defining model parameters (amount of users and movies) based on our data\n",
    "num_users, num_items = user_rating_matrix_np.shape\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def initialise_model(num_users= num_users, num_items=num_items, lr=0.03, wd=2.5e-5):\n",
    "    global model\n",
    "    model = Recommender(num_users, num_items)\n",
    "    model = model.to(device)\n",
    "    global optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=wd)\n",
    "\n",
    "initialise_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our model, and prepared the data for training, we can move onto model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs):\n",
    "    model.train()\n",
    "    loss_dict = {}\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0\n",
    "        for user, item, rating in train_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(user, item).squeeze()\n",
    "            loss = criterion(output, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print loss after every epoch\n",
    "        epoch_loss = total_loss/len(train_loader)      # Get the average loss for this epoch\n",
    "        loss_dict[epoch] = epoch_loss                  # Store this epoch, loss into a dict for graphing\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss}\")    # Print the average loss for this epoch\n",
    "    \n",
    "    return loss_dict                                       # Return the Loss Dict\n",
    "\n",
    "# loss_dict = training_loop(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(loss_dict)\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(range(1,epochs+1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing an elbow point around 5, so we will update epochs to 5 to mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in test_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            output = model(user, item).squeeze()\n",
    "            loss = criterion(output, rating)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    test_loss = total_loss / len(test_loader)   \n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    return test_loss\n",
    "\n",
    "validation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New epochs amount after analysis\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0005,0.001, 0.003, 0.005, 0.01, 0.013, 0.015]\n",
    "lr_dict = {'learning_rate': [], 'training_loss': [], 'test_loss': [], 'test_diff':[]}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Learning Rate: {lr} \\n\")\n",
    "    initialise_model(lr=lr)\n",
    "    training_loss = training_loop(epochs)[epochs]\n",
    "    test_loss = validation_loop()\n",
    "    \n",
    "    lr_dict['learning_rate'].append(lr)\n",
    "    lr_dict['training_loss'].append(training_loss)\n",
    "    lr_dict['test_loss'].append(test_loss)\n",
    "    lr_dict['test_diff'].append((test_loss/training_loss)-1)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_dict)\n",
    "lr_df.sort_values(by='test_diff', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4474934730288032\n",
      "Epoch 2, Loss: 1.04309966206759\n",
      "Epoch 3, Loss: 0.9795916969073819\n",
      "Epoch 4, Loss: 0.9320885766740445\n",
      "Test Loss: 0.9758448461049148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9758448461049148"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinitialise the model\n",
    "initialise_model(lr = 0.0005)\n",
    "\n",
    "# Retrain the model\n",
    "training_loop(epochs)\n",
    "\n",
    "validation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recieved a test loss much higher than our training loss. We overfit! We'll continue to tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratings_review(user_id, movie_ids):\n",
    "    model.eval()                                 # Set model to evaluation mode\n",
    "    user = torch.tensor([user_id] * len(movie_ids)).to(device)  # Ensure tensor is on the same device as the model\n",
    "    movies = torch.tensor(movie_ids).to(device)  # Ensure tensor is on the same device as the model\n",
    "    with torch.no_grad():\n",
    "        predicted_ratings = model(user, movies).squeeze().tolist()\n",
    "\n",
    "    return predicted_ratings\n",
    "\n",
    "ratings_list = []\n",
    "for id in ratings.userId.unique():\n",
    "        recommended_movies = ratings_review(id, movie_ids)\n",
    "        ratings_list.append(recommended_movies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(ratings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_series = pd.Series(ratings_list)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Original Data\n",
    "sns.boxplot(y=ratings['rating'], ax=axes[0], color='orange')\n",
    "axes[0].set_title('Original Ratings')\n",
    "axes[0].set_xlabel('Original')\n",
    "axes[0].set_ylabel('Rating')\n",
    "\n",
    "# Model Data\n",
    "sns.boxplot(y=ratings_series, ax=axes[1], color='blue')\n",
    "axes[1].set_title('Model Recommendations')\n",
    "axes[1].set_xlabel('Model')\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle('Boxplots of Original and Model Ratings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)\n",
    "xticks = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "bins = 10\n",
    "# Original Data\n",
    "sns.histplot(x=ratings['rating'], bins = bins, ax=axes[0], color='orange')\n",
    "axes[0].set_title('Original Ratings')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_xticks(xticks)\n",
    "\n",
    "# Model Data\n",
    "sns.histplot(x=ratings_series, bins = bins, ax=axes[1], color='blue')\n",
    "axes[1].set_title('Model Recommendations')\n",
    "axes[1].set_xlabel('Rating')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xticks(xticks)\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle('Histograms of Original and Model Ratings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_series = pd.Series(ratings_list)\n",
    "ratings_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, movie_ids, top_n=10):\n",
    "    model.eval()                                 # Set model to evaluation mode\n",
    "    user = torch.tensor([user_id] * len(movie_ids)).to(device)  # Ensure tensor is on the same device as the model\n",
    "    movies = torch.tensor(movie_ids).to(device)  # Ensure tensor is on the same device as the model\n",
    "    with torch.no_grad():\n",
    "        predicted_ratings = model(user, movies).squeeze()\n",
    "    top_movie_indices = predicted_ratings.argsort(descending=True)[:top_n]\n",
    "    recommended_movie_ids = movies[top_movie_indices].tolist()\n",
    "    recommended_ratings = predicted_ratings[top_movie_indices].tolist()\n",
    "    recommendations = list(zip([user_id] * top_n, recommended_movie_ids, recommended_ratings))\n",
    "    return recommendations\n",
    "\n",
    "# Example usage for recommendations\n",
    "recommended_movies = recommend_movies(5, movie_ids, top_n=1)\n",
    "\n",
    "\n",
    "print(f\"For User: {recommended_movies[0][0]}, \\\"{moviesid_to_title[recommended_movies[0][1]]}\\\" has an estimated rating of {recommended_movies[0][2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
